{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\n##Get the web page content\n\nDEBUG = False;",
      "execution_count": 72,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import logging\nimport smtplib\n\nlogging.basicConfig(filename='execution.log', level=logging.DEBUG, \n                    format='%(asctime)s %(levelname)s %(name)s %(message)s')\nlogger=logging.getLogger(__name__)\n\n#try:\n#    1/0\n#except ZeroDivisionError as err:\n#    logger.error(err)\n\n\n#EMAIL OK ######################################\nusername = str('perell2020@gmail.com')  \npassword = str('Zurich46744113')  \nserver = \"smtp.gmail.com\"\nsent_from = \"perell2020@gmail.com\"\nto = \"pcll1m@yahoo.es\"\nemail_body = \"Error:\"\nemail_subject = \"Problem executing scraping activity\"\n\n# send email\ndef send_email(user, pwd, recipient, subject, body):\n    FROM = user\n    TO = recipient if isinstance(recipient, list) else [recipient]\n    SUBJECT = subject\n    TEXT = body\n\n    # Prepare actual message\n    message = \"\"\"From: %s\\nTo: %s\\nSubject: %s\\n\\n%s\n    \"\"\" % (FROM, \", \".join(TO), SUBJECT, TEXT)\n    try:\n        server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n        server.ehlo()\n        server.starttls()\n        server.login(user, pwd)\n        server.sendmail(FROM, TO, message)\n        server.close()\n        print ('successfully sent the mail')\n    except:\n        print (\"failed to send mail\")\n        logger.error(\"failed to send mail\")\n\n\ndef reporterror(errordesc):\n    logger.error(errordesc)    \n    send_email('perell2020@gmail.com','Zurich46744113','pcll1m@yahoo.es','2019-nCov Python SCRAPE_BNO Error on exec',errordesc)",
      "execution_count": 66,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Libray\nimport requests\nfrom bs4 import BeautifulSoup as bs\nimport datetime\nimport dateutil.parser\n\n#url\nurl = \"https://bnonews.com/index.php/2020/02/the-latest-coronavirus-cases/\"\ndiscardlist = ['MAINLAND CHINA','INTERNATIONAL','REGIONS','']\n\n\n#return empty sring\ndef emptystr(s):\n    if s is None:\n        return ''\n    return str(s)\n\n#Control request access\ndef request_url(objurl):\n    #request\n    headers = {\n    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n    try:\n        # your logic is here\n        response = requests.get(url, headers=headers, timeout=5)\n        if response.status_code!=200:\n            print(\"Wrong status code: \" + response.status_code)\n            return \"Error status code\"\n        #Access the full response as text (get the HTML of the page in a big string)\n        #print (response.text)\n        #Look for a specific substring of text within the response\n        if \"blocked\" in response.text:\n            print (\"we've been blocked\")\n            return \"Error blocked\"\n    except requests.ConnectionError as e:\n        print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n        print(str(e))\n        return \"Error conn. error\"\n    except requests.Timeout as e:\n        print(\"OOPS!! Timeout Error\")\n        print(str(e))\n        return \"Error timeout\"\n    except requests.RequestException as e:\n        print(\"OOPS!! General Error\")\n        print(str(e))\n        return \"Error gral. error\"    \n    except KeyboardInterrupt:\n        print(\"Someone closed the program\")\n        return \"Error someone closed the program\"\n    return response.text\n        \n        \n#scrape function\ndef scrape (html_content):\n    try:\n        doc = ''\n        table_doc = bs(html_content, 'html.parser')\n        #get the urlfinal\n        urlfinal = '# source: BNO @ ' + emptystr(table_doc.find_all(\"link\", {\"rel\" : \"canonical\"})[0].get('href')) + '\\n'\n        #print (urlfinal)\n        #update\n        update = emptystr(table_doc.find_all('em')[0].string).replace('Last update:','') + '\\n'\n        if update !='':\n            #process date toi the right format\n            date_time_str = update\n            date_object = dateutil.parser.parse(date_time_str)\n            #INFOdate_string = ' 8 February 2020 at 5:17 p.m. ET\\n' does not match format '%d %B %Y %I:%M%p''\n            #date_object = datetime.datetime.strptime(date_time_str, '%d %B %Y %I:%M%p')\n            #print(\"DATE \" + str(date_object))\n            #date_time_obj = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S.%f')\n            #print('Date-time:', date_time_obj)\n        update = '# update:' + str(date_object) + ' ET\\n'    \n        # place|confirmed_cases|deaths|notes|sources\n        place = '# place|confirmed_cases|deaths|notes|sources\\n' \n        doc = urlfinal + update + place\n        firstPass = True\n        # parsing html content\n        for tables in table_doc.find_all('table'):\n            for tr in tables.find_all('tr'):\n                #ATTENTION USE .TEXT WHEN SPECIAL CHARS\n                rowControl = emptystr(tr.find_all('td')[0].text)\n                if rowControl not in discardlist:\n                    line = ''\n                    #control hubei\n                    if rowControl.find(\"Hubei\") != -1: \n                        #in 'Hubei': \n                        rowControl = 'Hubei'\n                    if (firstPass and rowControl in 'TOTAL'):\n                        rowControl = 'China'\n                        line = rowControl + '|' + emptystr(tr.find_all('td')[1].string) + '|' + emptystr(tr.find_all('td')[2].string) + '|' + emptystr(tr.find_all('td')[3].string) + '\\n'\n                    if (firstPass and rowControl not in 'TOTAL'):\n                        rowControl =  rowControl.replace('province','')\n                        source = emptystr(tr.find_all('td')[4].find('a'))\n                        if source!=\"\":\n                            source = emptystr(tr.find_all('td')[4].find('a').get('href'))\n                        line = rowControl + '|' + emptystr(tr.find_all('td')[1].string) + '|' + emptystr(tr.find_all('td')[2].string) + '|' + emptystr(tr.find_all('td')[3].string) + '|' +  source + '\\n'\n                    if (firstPass==False and rowControl not in 'TOTAL'):\n                        source = emptystr(tr.find_all('td')[4].find('a'))\n                        if source!=\"\":\n                            source = emptystr(tr.find_all('td')[4].find('a').get('href'))\n                        line = rowControl + '|' + emptystr(tr.find_all('td')[1].string) + '|' + emptystr(tr.find_all('td')[2].string) + '|' + emptystr(tr.find_all('td')[3].string) + '|' +  source + '\\n'\n                    doc = doc + line \n            firstPass = False         \n        #print(doc)\n        return(doc)\n    except AttributeError as e:\n        return(\"ERROR \" + str(e))\n    ",
      "execution_count": 73,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#OUTPUT MODEL\n# source: BNO @ https://bnonews.com/index.php/2020/01/the-latest-coronavirus-cases/\n# update: 2020-01-24 14:55:00 CST\n# place|confirmed_cases|deaths|notes|sources\n#Hubei|729|39|100 serious/57 critical|http://wjw.hubei.gov.cn/fbjd/dtyw/202001/t20200125_2014854.shtml\n#Guangdong|53|0|12 serious, 3 critical|http://wsjkw.gd.gov.cn/zwyw_yqxx/content/post_2878949.html\n#Zhejiang|43|0|6 serious|https://www.zjwjw.gov.cn/art/2020/1/24/art_1202101_41855167.html",
      "execution_count": 74,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#process from file\ndef scrapefromfile(fname):\n    try:\n        f = open(fname, 'rb')\n    except OSError:\n        print (\"Could not open/read file:\", fname)\n        sys.exit()\n\n    with f:\n        #reader = csv.reader(f)\n        #for row in reader:\n        #    pass #do stuff here\n        data = f.read()\n    \n    if data[0:4] !=\"Error\":\n        print ('Got response')\n        scrape(data)\n    else:\n        print('Problem accessing: Error code' + data)\n\n#scrapefromfile(\"bno_6.html\")\n\n#processfromweb\ndef scrapefromweb(url):\n    #If page available proceed with scrape\n    data = request_url(url)\n    if data[0:4] !=\"Error\":\n        print ('Got response')\n        output = scrape(data)\n        if output[0:4] !=\"Error\":\n            print('Scraping content went well')\n        else:\n            print('Problem Scraping: Error' + output)\n            reporterror('Problem Scraping: Error' + output)\n    else:\n        print('Problem accessing: Error code' + data)\n        reporterror('Problem Scraping: Error' + output)\n    return output\n                    \n        \n                    \ndatafile = \"data.txt\"\n                    \n\n#process from file\ndef readfile(fname):\n    try:\n        f = open(fname, 'rb')\n    except OSError:\n        print (\"Could not open/read file:\", fname)\n        reporterror('Could not open/read file:', fname)\n        #sys.exit()\n        datafile =''\n    with f:\n        datafile = f.read()\n    return datafile\n                    \n#append to file\ndef appendtofile(fname,datatowrite):\n    if DEBUG:\n        return True\n    try:\n        f = open(fname, 'a')\n    except OSError:\n        print (\"Could not open/read file:\", fname)\n        reporterror('Could not append to file:', fname)\n        #sys.exit()\n        return False\n    with f:\n        print('APPEND')\n        n = f.write(datatowrite)\n        f.close()\n        return True\n        \n                                                  \n#def testIfNeedScraping:\ndef doweneedtoscrape():\n    #open file                \n    content = readfile(datafile)              \n    #latest getdate data\n    #dataurl = 'https://raw.githubusercontent.com/perell2014/data_2019ncov/master/data.txt'\n    #response = requests.get(dataurl)\n    #content = response.text\n    blocks = str(content).split(\"update:\")\n    if (len(blocks)>0):\n        latestDate = blocks[len(blocks)-1][0:19]\n        date_latestDate = dateutil.parser.parse(latestDate)\n        #print (latestDate)\n        #get last update from the scrapping               \n        scrapedcontent = scrapefromweb(url)\n        if scrapedcontent[0:4] !=\"Error\":\n            blocks = scrapedcontent.split(\"update:\")\n            if (len(blocks)>0):\n                latestwebDate = blocks[len(blocks)-1][0:19]\n                date_latestwebDate = dateutil.parser.parse(latestwebDate)\n                #print (latestwebDate)\n                #CMP DATA\n                if (date_latestDate < date_latestwebDate):\n                    if appendtofile(datafile,scrapedcontent):\n                        logger.info('Successfull Append of the data file extraction with the last update:' + latestDate + \" web last update: \" + latestwebDate)\n                        print('Successfull Append of the data file extraction with the last update:' + latestDate + \" web last update: \" + latestwebDate)\n                        if DEBUG:\n                            print(scrapedcontent)\n                else:\n                    logger.info(\"No need to scrape data file last date:\" + latestDate + \" web last update:\" + latestwebDate)\n            else:\n                reporterror('No update section found in the scraping web')\n        else:\n            reporterror('No update date found in the data file!!')\n    \n    \n#Execution\nif __name__ == \"__main__\":\n    logger.info(\"Sarting process SCRAPE_NBO\")\n    doweneedtoscrape()\n         ",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Got response\nScraping content went well\nAPPEND\nSuccessfull Append of the data file extraction with the last update:2020-02-08 17:17:00 web last update: 2020-02-08 20:02:00\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!git status",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": "On branch master\nYour branch is ahead of 'origin/master' by 7 commits.\n  (use \"git push\" to publish your local commits)\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\n\n\t\u001b[31mmodified:   SCRAPE_BNO.ipynb\u001b[m\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!git add data.txt\n!git add execution.log\n!git add .ipynb_checkpoints/SCRAPE_BNO-checkpoint.ipynb\n!git add .ipynb_checkpoints/ETL-checkpoint.ipynb\n!git add bno_6.html\n!git add data_6_7_8.txt\n!git add SCRAPE_BNO.ipynb\n!git add ETL.ipynb\n!git commit -am \"Last 3\"",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[master 1e83851] Last 3\n 4 files changed, 95 insertions(+), 17 deletions(-)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!cd ~/library\n!git config user.name \"perell2014\"\n!git config user.email \"perell@gmail.com\"\n!git config -l\n!git config push.default simple\n!git push https://perell2014:aaaAAA2014@github.com/perell2014/data_2019ncov.git --all",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": "core.repositoryformatversion=0\ncore.filemode=true\ncore.bare=false\ncore.logallrefupdates=true\nremote.origin.url=https://github.com/perell2014/data_2019ncov\nremote.origin.fetch=+refs/heads/*:refs/remotes/origin/*\nbranch.master.remote=origin\nbranch.master.merge=refs/heads/master\nuser.name=perell2014\nuser.email=perell@gmail.com\npush.default=simple\nCounting objects: 12, done.\nDelta compression using up to 2 threads.\nCompressing objects: 100% (12/12), done.\nWriting objects: 100% (12/12), 8.61 KiB | 0 bytes/s, done.\nTotal 12 (delta 7), reused 0 (delta 0)\nremote: Resolving deltas: 100% (7/7), completed with 3 local objects.\u001b[K\nTo https://perell2014:aaaAAA2014@github.com/perell2014/data_2019ncov.git\n   5fda891..1e83851  master -> master\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}